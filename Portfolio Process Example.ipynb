{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNl9d46tZVSDsv1sCIJ0XGq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Software Development Lifecycle\n","### Requirement Gathering\n","\n","This process is currently managed via Excel VBA, using data collected from a system known as InvestOne. The requirement for the information is to transform it into reports that are in a format accepted by external companies, as well as in a way that is more useful for internal users.\n","\n","### Design\n","\n","The output for these reports are simple excel formats, with data summarised into several columns. There are two main versions of these reports, list of holdings (from SecurityDesc) and cash and fund values (from Fund Trend). The other version is similar, but also adds an additional column for holdings as a % of fund value. To streamline this process, we will implement one table which houses a concatenation of data from both SecuirtyDesc and FundTrend, adding in the % column, to then be further transfromed into the final reports."],"metadata":{"id":"H4oyCVVOOJHH"}},{"cell_type":"markdown","source":["# Important Note\n","\n","### As this report involves confidential data, sample datasets cannot be provided. The code should be evaluated on its quality, readability, and implementation approach."],"metadata":{"id":"zdk0BXylpc3e"}},{"cell_type":"markdown","source":["### Implementation"],"metadata":{"id":"Kzp7CzVOOqHr"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"BM8QT4uIOya1"}},{"cell_type":"code","source":["import pandas as pd\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from notebookutils import fs"],"metadata":{"id":"XofpsPILO3UL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get Data"],"metadata":{"id":"FoPehsaWO7iu"}},{"cell_type":"code","source":["# Gathered data from a OneLake lakehouse using pyspark SQL functions and converts them to pandas to be amended via Python\n","dfsec = spark.sql(\"SELECT * FROM Marlborough_Lakehouse.brz_securitydesc\").toPandas()\n","dftren = spark.sql(\"SELECT * FROM Marlborough_Lakehouse.brz_fundtrendclass\").toPandas()"],"metadata":{"id":"Qyw8rrpdO9Z5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Remove Unnecessary Columns"],"metadata":{"id":"SEMxKUZCPDqW"}},{"cell_type":"code","source":["# Trim each report down to only the columns required for the final report\n","dfsec = dfsec[['Security_Number',\n","'Shares_Par',\n","'Security_Description_Short',\n","'Strike_Price',\n","'Traded_Market_Value_Base',\n","'Trading_Currency',\n","'Position_Date',\n","'Account_Name',\n","'Security_ISIN']]\n","\n","dftren = dftren[['Account_Name',\n","'Net_Income_Amount',\n","'OEIC_Uninvested_Principal_Cash_Bid',\n","'OEIC_Security_Value',\n","'Cancellation_Value',\n","'Net_Unrealized_G/L_On_FEC',\n","'Account_Class',\n","'Date_As_Date']]"],"metadata":{"id":"fdNTcLDqPNSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Process Data"],"metadata":{"id":"2ZjYEOuYn9TJ"}},{"cell_type":"code","source":["#Rename column from dftren to match dfsec, for easier filtering\n","dftren.rename(columns={\"Date_As_Date\": \"Position_Date\"}, inplace=True)\n","\n","#Filter dftren to only account class 0, which is the total AUM for each fund.\n","dftren = dftren[dftren['Account_Class'] == 0]\n","\n","# Merge dfsec with a subset of dftren on 'Account_Number' and 'Position_Date'\n","# This brings in 'Cancellation_Value' from dftren where matching keys exist\n","# Code generated using information from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n","merged_df = dfsec.merge(dftren[[\"Account_Number\",\"Position_Date\",'Cancellation_Value']], on=[\"Account_Number\",\"Position_Date\"], how=\"left\")\n","\n","#Calcualte the AUM percentage for each holding within dfsec, using the AUM from dftren (now combined)\n","merged_df['%'] = merged_df['Traded_Market_Value_Base'] / merged_df['Cancellation_Value']\n","\n","# Set variables for funds and output to be used in a loop\n","funds = merged_df['Account_Name'].unique() # Will need to add testing here once mapping is available to ensure all desired accounts are captured.\n","output = pd.DataFrame()\n","\n","# Using a f string to create the final output in the desired format and loop through each fund\n","for f in funds:\n","    # Loads each fund by account name into the output dataframe\n","    output = pd.concat([output, merged_df.loc[merged_df['Account_Name'] == f]], ignore_index=True)\n","\n","    # Store the AUM value to be added into the summary row\n","    aum = merged_df.loc[merged_df['Account_Name'] == f, 'Cancellation_Value'].iloc[0]\n","\n","    # Add a mostly blank summary row after each list of securities with the total AUM, as required in the final output\n","    summary = pd.DataFrame(columns=merged_df.columns)\n","    summary.loc[0, 'Account_Name'] = f\n","    summary.loc[0, 'Cancellation_Value'] = aum\n","\n","    # Brings the final output together, each value per Account Name from the original dfsec table, summerised by the total AUM from the dftren table.\n","    output = pd.concat([output, summary])"],"metadata":{"id":"8Ld-mYXFoE5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load to new table within the Lakehouse\n","\n","### For completeness only, do not review"],"metadata":{"id":"xikHJkDwzJ2V"}},{"cell_type":"code","source":["# Default table upload process used within Fabric for the organisation\n","# Student did not write this code manually, only added it and amended it to show the final step in the process\n","\n","spark_df = spark.createDataFrame(output)\n","spark_df.withColumn('processed_date',current_timestamp()).write.mode('overwrite').format('delta').saveAsTable('daily_portfolios')"],"metadata":{"id":"MN0l5rVNzPSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Deployment and Maintenance (Future Scope)\n","\n","Whilst the transformation is available, due to the hard coded elements which will need to be developed and called upon from elsewhere within the data environment, this process cannot yet be deployed. When it can, it will done seamlessly, as the output should match the historic files already produced in the manual/VBA process.\n","\n","This process also currently runs and overwrites all data daily. Whilst this works short term, this will need to be updated long term to only process new data as it arrives."],"metadata":{"id":"l2cdKBtKn38n"}}]}